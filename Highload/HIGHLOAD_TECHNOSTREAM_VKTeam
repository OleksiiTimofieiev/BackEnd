https://www.youtube.com/watch?v=_d38g1tpLd8&list=PLrCZzMib1e9rZohs_FJg8MK52Ey494z40&index=7

///// Lecture 1: {
    - доступность
    - маштабируемость
}
///// Lecture 2: {
    - fat client: state is being saved on the client side
    pros: saving on traffic, delays, cache, constant connection, pushing (server sends events to client), offline mode, server without a state
    cons: update, different api versions, consistency, localization, experiments
    - thin client: state on server
    - session of user:
    authorization, coockies (id from server to client), memory on server, replication
    - fat web client: AJAX, Single Page Application
    - typical architecture:
    DB server
    Logic server
    Web Server 
    - resources: memory (structures to save data, access to memory), processor (blocks, system tasks, waiting, multythreading), disk, network
    - scaling: different servers, functions, sharding, partitioning, increasing quantity of replics
    - идемпотентность
    - RDBMS - ACID: atomicity, consistency, isolation, durability
    - optimizations: 
    index (disk operation are very costly and slow), 
    short transactions (one transaction waits another), 
    denormalization, 
    minimum logic, 
    minimum foreign key logic
    - CAP: consistency, availability, partition tolerance
    - master-slave: read from master and slave, write only to master, replication: sync, async
    - quorum
    - my read from master, others from slave
}
///// Lecture 4: Network stack {
    - 7 layers -> 4 layers
    Steps of HTTP request:
    1. DNS
    2. IP >> route => not in local network -> default gateway
    3. Route IP >> MAC == ARP Protocol
    - NIC: network interface controller. Configuring: ethtool
    - network stack is a huge program in OS
    - connection parameters:
    SO_KEEPALIVE
    SO_REUSEADDR
    SO_REUSEPORT
    TCP_CORK - данные пачками -> лучше througout
    TCP_NODELAY - as much faster as possible -> latency
    TCP_DEFER_ACCEP - don`t wake up until real data
    - get packet from NIC:
    user space network stack: DPDK, netmap
    XDP == express data path
    - RTT - round trip time
    - MTU == maximum transmission unit == 1500 bytes
    - MSS == payload
}
///// Lecture 5: balancing {
    - Master - Standby
    - FQDN == fully qualified domain name => Round-Robin DNS
    - L4 (TCP/UDP): balancing type (balancing of TCP connections) = LVS (linux virtual server):
    virtual ip address
    methods:
    1. assymetric routing: client -> L4 balancer -> direct reply to client, bonding, lo == loopback. bond -> lo -> app on 80 port
    2. L2 == ipip == packet in packet to machine in other network, 1500 => fragmentation (+8-10 GB of memory),
    not works: path MTU discovery
    works: advmss(limit to 1480 bytes -> left space for 20 bytes [ipip])
    3. NAT == Network Address Translation:
    client -> balancer -> server -> balancer -> client
    load on balancer, huge configuration issues
    4. high availability:
    L4 Balancer [balancing & monitoring]: Randon, Round-Robin, WRR (weighted round-robin, good to close connections - TCP aspect), WLC (weighted least connections), SH (Source Hash)
    5. upgrade/update: 
    6. Reliability: VRRP: two balancers or more (ping each other)
    7. Router: better to use source hash = ECMP == equal cost multipath 
    - DNS/GSLB (Global Server Load Balancing):
    TTL: time to leave = little TTL to check the server availability
    CDN == content delivery network:
    unicast: ip address prefix == ip network == ip address and mask => describe user -> cdn ===> from prefixes => go there
    anycast: 
    8. Load:
    To solve SYN flood:
    User-Space Network stack -> DPDK, Netmap, OpenLoad, pf_ring *100 speed 
    slow GET/POST:
    can overload the server/sql server => proxy[nginx, HAProxy, squid, APACHE]
    solution: post of a huge file, proxy will accumulate an then throws it to server
    gzip BOMB (compress/decompress on the fly):
    solution: proxy of L7
}
///// Lecture 6: processor and memory {
    - von neuman architecture
    - harward architecture: bus of command and data
    - cpu: 
    non-conveyor architecture, 
    conveyor architecture (every element of the conveyor can work independently),
    superscalar (instrucion dispetcher)
    - caching: exclusive or not exclusive
    - CPU -> L1 -> L2 ->L3 -> RAM: if cache miss
    - external instruction set (MMX, AES)
    - multicore architecture
    - cache coherency => 
    paralellism:
    Symmetric multiprocessing or shared-memory multiprocessing[1] (SMP)
    NUMA
    - memory has size of 4kb pages, hugepages: 2Mb, 1Gb
    - processes work with virtual memory & swap
    - resources have to be at ratinal maximum
    stack
    mmap
    heap
    bss (uninitialized data)
    init data (initialized data)
    text (program code)
    - files: /proc/meminfo, /proc/<pid>/status, /proc/<pid>/maps, /proc/vmstat
    - utilities: free, top, ps, perf, numactl
    - VMA == virtual memory area
}
///// Lecture 7: data storage {
    - parameters: volume, perfomance[normal, in degradation], latency, reliability, price
    - HDD: low price, bigger volume | huge latency, power consumtion, not reliable on physical interference
    - SSD: versus HDD -> block model of writing (read to buffer/delete/change/write)
    TRIM sometimes has more priority then I/O operations
    Garbage Collector: may clean all which is not necessary
    Wear leveling: increase life of disks == balances degradation
    Over provisioning:
    - SATA/SCSI
    - disk perfomance:
    HDD: length of seek operations
    SDD: paralellism
    - disk elevators(schedulers) == change I/O operation conveyor to optimize those operations:
    noop - FIFO (unite of one type requests)
    deadline - trying to provide specified latency, two queues on read/write
    cfg - as evenly as possible
    bfq - based on cfg, but more optimally
    - disk state: SMART std 
    - partitions: MBR/GPT
    - inode == index descriptor
    - fs: journaling, directories
    - RAID
}
///// Lecture 8: JVM in HighLoad {
    - why Java: runtime exceptions, cross-platform, easy
} 
///// Lecture 10: clouds {
    - capacity virtualization (OracleVM, VMWare)
    - virtual machine: 
    applicatins, 
    guets os, 
    virtual hardware,
    hypervisor 
}
///// Lecture 14: SRE {
    - inner problems of the service {
        - thread pool (ThreadPoolExecutor in Java, for example):
            out-of-memory == limits to create threads == available processors
            io-bound: 10 000 ros, ~20 ms each, 10000 rps * 1/50 sec = 200 request in system on each moment,
            about 200 threads
            > 10000 threads == could be problems
            queue of task can be huge => out-of-memory
            work out the timeouted tasked (user closed the browser) => limit the input queuer(Blocking queue)
            cancel request on some circumstances:
            503 == service unavailable
            remove old task, add new
            best practices:
                limit quantity of threads
                limit the size of the queue 
                limit the wait in the queue
                monitor: size of the queue, time being in queue, quantity of working threads, time of execution depending on requests
                no get without timeout, hierarchy of threadpool
                semaphore -> limit quantity of threads
        - connection pool:{
            same as for thread pool
        }
        - fail fast {
            acceptor -> selector -> worker
            - balancer: limit rps (on each server 300 rps)
            - request/sec != capacity
            - 503 == no free workers
            - faster => 503 on acceptor level
        }
        - slow start {
            JIT
            class loading
            empty local caches
            - warming of cache (be careful)
            - better to have good balancing politics and persistent cache
            - slowly reduce 503 requests
        }
        - graceful shutdown {
            1. close port for new connections
            2. close idle connections
            3. stop/shutdown thread pool
            4. waiting for all requests
            5. kill other
        }
    }
    - problems of inter service level {
        how many instances of the service are necessary: 3 or more(depends)
        timeouts (client/server):
            connection timeout [
                server is off
                port is closed
                backlog is full
            ]
            how to choose timeout: 
                long timeout == long time to find a problem. blocking of the resources
                fast: false timeouts
                limit quantity of retries
                503 can be retried, 502 - not
                retry of a slow request overloads the backend
                reject requests which we won`t be able to process
                exponential backoff + jitter on client
} 
///// Lecture 15: modern data structures {
    - streaming nature of data processing, no random access, uncontrollable incoming rate
    - solution: small data structures, more smaller - less accurate, ubiquitous hashing
    count-distinct (how many different items in dataset):
    classic == sort and run == used almost in all DB, spark approxCount Distinct, allmost all DB
    ~ linear counting => reduces memory used => database query optimizations
    BitSet mask = new BitSet(m)
    int position = hash(value)
    mask.set(position)
    ~ LogLog Counter(how many different item sin dataset):
    provide hash in binary == strings of 1010101 => binary division by  rang
    ~ count-min sketch(how often element appears in dataset):
    generic == multiset, hashmap, values + counters
    minimum on results of hash function(sometimes we have conflicts)
    depth x width
    use: picking safe password, natural language processing, heavy hitters identifying
    ~ find top-k element in dataset
    ~ space-saving:
    content-delivery networks, traffic monitoring, anomaly detection
    count-min sketch + heap
    ~ membership (does element belongs to dataset):
    generic: set/array =? O(n)
    Bloom Filter used:
    use: reducing exact checks/lookups, widly used in DB, prevent one-hit-wonders from caching,
    avoid show already shown items to user
    ~ sliding window
}
///// Lecture 16: replication {
    - replication (master-master) with leader (master-slave == one leader, many follower, kafka, databases) 
    {
        sync == confiramation when operation finished on slave, 
            pros: full data copy, data availability when master fails
            cons: one slave fails -> all fails, blocked writes
        async == not waiting for the confirmation, remedy to have the normal latency
            pros: one slave fails -> all fails, master not fails
            cons: potential data loss
        semi-sync: some slaves in sync (2 slaves at least), others in async
        - way to hadle the slave:
        get consistence snapshot from the leader, move it to follower,
        after follower gets full snapshot, then asking data after snapshot,
        then follower is being in catch up state
        - multiversion concurrency control
        - failover process == if leader fails -> choose another: select good candidate:
        are you volunteer -> request from client -> others choosing replica as leader(maybe sync replica?)
        to detect failover == timeout, heartbeats
        election process(choosing by majority consenus: PAXOS) <= appointing by controller node
        potential problems:
        1. lost writes
        2. discarded lost writes: reusing auto-increment primary keys
        3. split brain: two nodes think that they are being master, 
        both leaders accept writes, no conflict resolution, corrupted or lost data
        4. correct timeout:
        longer timeout -> longer time to recovery
        shorter timeout -> unnecessary failover processes
        - replication log implementations:
        statement-based (requests which came to master) => every request sent to followers "as-is"
            problems:
                lightweight trafic
                non-deterministic functions (NOW(), RAND())
                auto-increments, UPDATE WHERE
                state side effects
                default in old version on MySQL
        write-ahead-log shipping(physical) == write to log then apply
            journal containes description of changes on low level
            cons:
                every storage modifications is written to WAL (on leader)
                very low level
                potential downtime for upgrades
                storage format conflicts
                unused for external systems
        logical log replication == different log formats for replication and storage engine, working with ids of writes
            tips:
                possibly lots of traffic
                different software version on master in slave
                zero-downtime for upgrades
                change data capture
        - read-scaling architecture (read a lot, write lesser):
        leader-based replication
        suatable for web
        scaling by adding followers
        needs a lot of async replication
        => eventual consitency: read from async may be outdated, inconsistency is temprary, followers eventually become as is leader
        - replication lag anomally:
        read-on-writes (reading from replica not in sync with master)
            solutions:
                read write from master(leader) user profile management
                tracking time of last update <= our max replication delay
                monitor replication lags and prevent some routings (
                    route to up-to-date follower or wait to catch up
                )
        monotonic reads (user sees writes in non monotonic order)
            solutions:
                each user reads always from the same replica(based on user ID)
        consistent prefix reads (reply before question) == casuality violations, typical for partitioned databases
            solutions:
                consistent prefix reads
                casually related writes to the same partition
    }
    - multi-leader replication {
        - usually for data centers
        - two leaders communication from leader stream
        - all followers listen to all leaders
        - conflict resolver beetween data centers
        pros:
            perfomce(geographical)
            DC outage tolerance
            unreliable network tolerance (temporary network issues don`t affect on writes)
        cons:
            concurrent modification of the same data => inevitable conflict resolution
            interactions with auto increments , triggers, integrity constants
        - dangerouts, should be avoided as possible
        - offline clients: calendar app, ability to read and write without network connection, synching changes beetween devices(hours and days replication lag)
        - collaborative replication: google docs, shared links
        solutions:
            conflict avoidance
                all writes for particular record through the same leader (designate home leader for each user)
                problem: designated leader change
            converging towards the consistent state
                - no writes ordering
                - data eventually same on all replicas
                resolution:
                    unique id: pick with the highest ID
                    give each replica unique id. pick the highest id
                    merge conflicted values
                    custom:
                        on write (call very lightweight conflict handler on write)
                        on read (all conflicted )
                    automatic conflict resolution:
                        conflict free replicated data type(sets, lists, apps)
                        mergeable immutable data structures(3-way merge. track history)
                    operational transformation:             
    }   
    - leader-less replication {
        - clients send request to single node 
        - databases cares about replication
        - leader determines write order
        - any replica can access writes
        - direct interaction with replica vs coordinator
        convergence:
            read-repair (good for frequently read values)
            anti-entropy background process (find inconsistency and fix it)
            quorums:
                n replicas
                w confirmations for successful write
                r read responses
                (r + w) > n => read up to date values
                n,w,r are confirable == cassandra - consistency level
                w < n = write tolerate nodes unavailability
                r < n = read tolerates nodes unavailability
                n = 3, w = 2, r = 2 => tolerate one unavailable node
                limitations:
                    concurrent writes => potential data loss
                    concurrent write => undetermined read behavior
                    failed writes (< w) = undetermined read
                    sloppy quorum - may not r and w nodes overlapping
                pros: 
                    high availability
                    low latency
                    stale read tolerance
    }
}